{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Language modeling involves predicting the next word in a sequence given the sequence of words already present. A language model is a key element in many natural language processing models such as machine translation and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: One - Word - In, One - Word - Out Sequences\n",
    "\n",
    "We can start with a very simple model. Given one word as input, the model will learn to predict\n",
    "the next word in the sequence. For example:\n",
    "- X, y\n",
    "- Jack, and\n",
    "- and, Jill\n",
    "- Jill, went"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source text\n",
    "data = \"\"\" Jack and Jill went up the hill\\n\n",
    "To fetch a pail of water\\n\n",
    "Jack fell down and broke his crown\\n\n",
    "And Jill came tumbling after\\n \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first step is to encode the text as integers. Each lowercase word in the source text is assigned a unique integer and we can convert the sequences of words to sequences of integers. Keras provides the Tokenizer class that can be used to perform this encoding. First, the Tokenizer is fit on the source text to develop the mapping from words to unique integers. Then sequences of text can be converted to sequences of integers by calling the texts_to_sequences() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Jack and Jill went up the hill\\n\\nTo fetch a pail of water\\n\\nJack fell down and broke his crown\\n\\nAnd Jill came tumbling after\\n ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Jack and Jill went up the hill\\n\\nTo fetch a pail of water\\n\\nJack fell down and broke his crown\\n\\nAnd Jill came tumbling after\\n '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 2,\n",
       " 14,\n",
       " 15,\n",
       " 1,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 1,\n",
       " 3,\n",
       " 19,\n",
       " 20,\n",
       " 21]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# integer encode text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 22\n"
     ]
    }
   ],
   "source": [
    "# determine the vocabulary size (unique words)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Running above example, we can see that the size of the vocabulary is 21 words. We add one, because we will need to specify the integer for the largest encoded word as an array index, e.g. words encoded 1 to 21 with array indicies 0 to 21 or 22 positions. Next, we need to create sequences of words to fit the model with one word as input and one word as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 24\n",
      "[[2, 1], [1, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11], [11, 12], [12, 13], [13, 2], [2, 14], [14, 15], [15, 1], [1, 16], [16, 17], [17, 18], [18, 1], [1, 3], [3, 19], [19, 20], [20, 21]]\n"
     ]
    }
   ],
   "source": [
    "# create word -> word sequences\n",
    "sequences = []\n",
    "for i in range(1,len(encoded)):\n",
    "    sequence = encoded[i-1:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1],\n",
       "       [ 1,  3],\n",
       "       [ 3,  4],\n",
       "       [ 4,  5],\n",
       "       [ 5,  6],\n",
       "       [ 6,  7],\n",
       "       [ 7,  8],\n",
       "       [ 8,  9],\n",
       "       [ 9, 10],\n",
       "       [10, 11],\n",
       "       [11, 12],\n",
       "       [12, 13],\n",
       "       [13,  2],\n",
       "       [ 2, 14],\n",
       "       [14, 15],\n",
       "       [15,  1],\n",
       "       [ 1, 16],\n",
       "       [16, 17],\n",
       "       [17, 18],\n",
       "       [18,  1],\n",
       "       [ 1,  3],\n",
       "       [ 3, 19],\n",
       "       [19, 20],\n",
       "       [20, 21]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import array\n",
    "# split into X and y elements\n",
    "sequences = array(sequences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  2, 14, 15,  1,\n",
       "       16, 17, 18,  1,  3, 19, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sequences[:,0]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  2, 14, 15,  1, 16,\n",
       "       17, 18,  1,  3, 19, 20, 21])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sequences[:,1]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will fit our model to predict a probability distribution across all words in the vocabulary. That means that we need to turn the output element from a single integer into a one hot encoding with a 0 for every word in the vocabulary and a 1 for the actual word that the value.This gives the network a ground truth to aim for from which we can calculate error and update the model. Keras provides the to_categorical() function that we can use to convert the integer to a one hot encoding while specifying the number of classes as the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "# one hot encode outputs\n",
    "y = to_categorical(y,num_classes = vocab_size)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 10)             220       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 22)                1122      \n",
      "=================================================================\n",
      "Total params: 13,542\n",
      "Trainable params: 13,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\alekh\\AppData\\Local\\conda\\conda\\envs\\Tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alekh\\AppData\\Local\\conda\\conda\\envs\\Tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1dbeaf00bc8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# define the model\n",
    "def define_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,10,input_length = 1))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(vocab_size,activation = 'softmax'))\n",
    "    # compile network\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "# define model\n",
    "model = define_model(vocab_size)\n",
    "# fit network\n",
    "model.fit(X,y,epochs = 500, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "# evaluate \n",
    "in_text = 'Jack'\n",
    "print(in_text)\n",
    "encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "encoded = array(encoded)\n",
    "yhat = model.predict_classes(encoded,verbose = 0)\n",
    "for word,index in tokenizer.word_index.items():\n",
    "    if index == yhat:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('and', 1), ('jack', 2), ('jill', 3), ('went', 4), ('up', 5), ('the', 6), ('hill', 7), ('to', 8), ('fetch', 9), ('a', 10), ('pail', 11), ('of', 12), ('water', 13), ('fell', 14), ('down', 15), ('broke', 16), ('his', 17), ('crown', 18), ('came', 19), ('tumbling', 20), ('after', 21)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model,tokenizer,seed_text,n_words):\n",
    "    in_text,result = seed_text,seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = array(encoded)\n",
    "        # predict a word in the vocabulary\n",
    "        yhat = model.predict_classes(encoded,verbose = 0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == yhat :\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text,result = out_word, result + ' ' + out_word\n",
    "        print(in_text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "jill\n",
      "came\n",
      "tumbling\n",
      "after\n",
      "hill\n",
      "Jack and jill came tumbling after hill\n"
     ]
    }
   ],
   "source": [
    "print(generate_seq(model,tokenizer,'Jack',6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "came tumbling after\n"
     ]
    }
   ],
   "source": [
    "print(generate_seq(model,tokenizer,'came',2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Line-by-Line Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,                                     y\n",
    "# _, _, _, _, _, Jack,                   and\n",
    "# _, _, _, _, Jack, and,                 Jill\n",
    "# _, _, _, Jack, and, Jill,              went\n",
    "# _, _, Jack, and, Jill, went,           up\n",
    "# _, Jack, and, Jill, went, up,          the\n",
    "# Jack, and, Jill, went, up, the,        hill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This approach may allow the model to use the context of each line to help the model in those cases where a simple one-word-in-and-out model creates ambiguity. In this case, this comes at the cost of predicting words across lines, which might be fine for now if we are only interested in modeling and generating lines of text. Note that in this representation, we will require a padding of sequences to ensure they meet a fixed length input. This is a requirement when using Keras. First, we can create the sequences of integers, line-by-line by using the Tokenizer already fit on the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Jack and Jill went up the hill\\n\\nTo fetch a pail of water\\n\\nJack fell down and broke his crown\\n\\nAnd Jill came tumbling after\\n '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Jack and Jill went up the hill\\n\\nTo fetch a pail of water\\n\\nJack fell down and broke his crown\\n\\nAnd Jill came tumbling after\\n ']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Jack and Jill went up the hill\\n\\nTo fetch a pail of water\\n\\nJack fell down and broke his crown\\n\\nAnd Jill came tumbling after\\n '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jack and Jill went up the hill\n",
      "[2, 1, 3, 4, 5, 6, 7]\n",
      "[2, 1]\n",
      "[2, 1, 3]\n",
      "[2, 1, 3, 4]\n",
      "[2, 1, 3, 4, 5]\n",
      "[2, 1, 3, 4, 5, 6]\n",
      "[2, 1, 3, 4, 5, 6, 7]\n",
      "\n",
      "[]\n",
      "To fetch a pail of water\n",
      "[8, 9, 10, 11, 12, 13]\n",
      "[8, 9]\n",
      "[8, 9, 10]\n",
      "[8, 9, 10, 11]\n",
      "[8, 9, 10, 11, 12]\n",
      "[8, 9, 10, 11, 12, 13]\n",
      "\n",
      "[]\n",
      "Jack fell down and broke his crown\n",
      "[2, 14, 15, 1, 16, 17, 18]\n",
      "[2, 14]\n",
      "[2, 14, 15]\n",
      "[2, 14, 15, 1]\n",
      "[2, 14, 15, 1, 16]\n",
      "[2, 14, 15, 1, 16, 17]\n",
      "[2, 14, 15, 1, 16, 17, 18]\n",
      "\n",
      "[]\n",
      "And Jill came tumbling after\n",
      "[1, 3, 19, 20, 21]\n",
      "[1, 3]\n",
      "[1, 3, 19]\n",
      "[1, 3, 19, 20]\n",
      "[1, 3, 19, 20, 21]\n",
      " \n",
      "[]\n",
      "Total Sequences: 21\n"
     ]
    }
   ],
   "source": [
    "# create line-based sequence\n",
    "sequences = list()\n",
    "for line in data.split('\\n'):\n",
    "    print(line)\n",
    "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
    "    print(encoded)\n",
    "    for i in range(1,len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        print(sequence)\n",
    "        sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1],\n",
       " [2, 1, 3],\n",
       " [2, 1, 3, 4],\n",
       " [2, 1, 3, 4, 5],\n",
       " [2, 1, 3, 4, 5, 6],\n",
       " [2, 1, 3, 4, 5, 6, 7],\n",
       " [8, 9],\n",
       " [8, 9, 10],\n",
       " [8, 9, 10, 11],\n",
       " [8, 9, 10, 11, 12],\n",
       " [8, 9, 10, 11, 12, 13],\n",
       " [2, 14],\n",
       " [2, 14, 15],\n",
       " [2, 14, 15, 1],\n",
       " [2, 14, 15, 1, 16],\n",
       " [2, 14, 15, 1, 16, 17],\n",
       " [2, 14, 15, 1, 16, 17, 18],\n",
       " [1, 3],\n",
       " [1, 3, 19],\n",
       " [1, 3, 19, 20],\n",
       " [1, 3, 19, 20, 21]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 7\n"
     ]
    }
   ],
   "source": [
    "# pad input sequences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences,maxlen = max_length,padding = 'pre')\n",
    "print('Max Sequence Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  2,  1],\n",
       "       [ 0,  0,  0,  0,  2,  1,  3],\n",
       "       [ 0,  0,  0,  2,  1,  3,  4],\n",
       "       [ 0,  0,  2,  1,  3,  4,  5],\n",
       "       [ 0,  2,  1,  3,  4,  5,  6],\n",
       "       [ 2,  1,  3,  4,  5,  6,  7],\n",
       "       [ 0,  0,  0,  0,  0,  8,  9],\n",
       "       [ 0,  0,  0,  0,  8,  9, 10],\n",
       "       [ 0,  0,  0,  8,  9, 10, 11],\n",
       "       [ 0,  0,  8,  9, 10, 11, 12],\n",
       "       [ 0,  8,  9, 10, 11, 12, 13],\n",
       "       [ 0,  0,  0,  0,  0,  2, 14],\n",
       "       [ 0,  0,  0,  0,  2, 14, 15],\n",
       "       [ 0,  0,  0,  2, 14, 15,  1],\n",
       "       [ 0,  0,  2, 14, 15,  1, 16],\n",
       "       [ 0,  2, 14, 15,  1, 16, 17],\n",
       "       [ 2, 14, 15,  1, 16, 17, 18],\n",
       "       [ 0,  0,  0,  0,  0,  1,  3],\n",
       "       [ 0,  0,  0,  0,  1,  3, 19],\n",
       "       [ 0,  0,  0,  1,  3, 19, 20],\n",
       "       [ 0,  0,  1,  3, 19, 20, 21]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y,num_classes = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    # compile network\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pre-pad sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "    # append to input\n",
    "    in_text += ' ' + out_word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 6, 10)             220       \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 22)                1122      \n",
      "=================================================================\n",
      "Total params: 13,542\n",
      "Trainable params: 13,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1dbf01f5a48>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# fit network\n",
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack fell\n",
      "Jill jill\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'Jack', 4))\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'Jill', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
