{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- text_to_word_sequence() function that you can use to split text into a list of words. By default, this function automatically does 3 things:\n",
    "    - Splits words by space.\n",
    "    - Filters out punctuation.\n",
    "    - Converts text to lowercase (lower=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "text = 'The quick brown fox jumped over the lazy dog!!.'\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot encode a text into a list of word indexes in a vocabulary of size n.\n",
    "\n",
    "- Return: List of integers in [1, n]. Each integer encodes a word (unicity non-guaranteed).\n",
    "\n",
    "- Arguments: Same as text_to_word_sequence above.\n",
    "\n",
    "     - n: int. Size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 16, 18, 19, 13, 19, 5, 3, 12]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "text1 = 'The quick brown fox jumped over the lazy dog!!.'\n",
    "result1 = one_hot(text1,20)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class for vectorizing texts, or/and turning texts into sequences (=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\n",
    "\n",
    "- Arguments: Same as text_to_word_sequence above.\n",
    "\n",
    "    - nb_words: None or int. Maximum number of words to work with (if set, tokenization will be restricted to the top nb_words most common words in the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_on_texts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Arguments:\n",
    "    - texts: list of texts to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "t  = Tokenizer()\n",
    "fit_text = [\"The earth is an awesome place live\"] # it will be treated as vocabulary of unique words and integer numbers will be\n",
    "                                                  # applied. Ex: the = 1, earth = 2, is = 3, an = 4, awesome = 5, place = 6,live = 7\n",
    "t.fit_on_texts(fit_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### texts_to_sequences(texts)\n",
    "\n",
    "- Arguments:\n",
    "    - texts: list of texts to turn to sequences.\n",
    "    - Return: list of sequences (one per text input). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences :  [[1, 2, 3, 4, 6, 7], [1, 3]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Similarly, list of sentences/single sentence in a list must be passed into texts_to_sequences.\n",
    "test_text1 = \"The earth is an great place live\"\n",
    "test_text2 = \"The is my program\"\n",
    "sequences = t.texts_to_sequences([test_text1, test_text2])\n",
    "\n",
    "print('sequences : ',sequences,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- corpus defined integers for given train data : the = 1, earth = 2, is = 3, an = 4, awesome = 5, place = 6,live = 7\n",
    "- if we look at test_text1 we have new word 'great' because this word not present in corpus, no integer number assigned in train data that is the reason in output also for that number is skipped.\n",
    "- like wise test_text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### texts_to_sequences_generator(texts): generator version of the above.\n",
    "\n",
    "- Return: yield one sequence per input text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# texts_to_matrix(texts):\n",
    "\n",
    "- Return: numpy array of shape (len(texts), nb_words).\n",
    "- Arguments:\n",
    "    - texts: list of texts to vectorize.\n",
    "    - mode: one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "t  = Tokenizer()\n",
    "docs = [\"The earth is an awesome place live\"] # it will be treated as vocabulary of unique words and integer numbers will be\n",
    "                                                  # applied. Ex: the = 1, earth = 2, is = 3, an = 4, awesome = 5, place = 6,live = 7\n",
    "t.fit_on_texts(docs)    \n",
    "encoded_docs = t.texts_to_matrix(docs,mode = 'count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit_on_sequences(sequences):\n",
    "\n",
    "- Arguments:\n",
    "    - sequences: list of sequences to train on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sequences_to_matrix(sequences):\n",
    "\n",
    "- Return: numpy array of shape (len(sequences), nb_words).\n",
    "- Arguments:\n",
    "    - sequences: list of sequences to vectorize.\n",
    "    - mode: one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributes:\n",
    "\n",
    "- word_counts: dictionary mapping words (str) to the number of times they appeared on during fit. Only set after fit_on_texts was called.\n",
    "- word_docs: dictionary mapping words (str) to the number of documents/texts they appeared on during fit. Only set after fit_on_texts was called.\n",
    "- word_index: dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called.\n",
    "- document_count: int. Number of documents (texts/sequences) the tokenizer was trained on. Only set after fit_on_texts or fit_on_sequences was called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n"
     ]
    }
   ],
   "source": [
    "docs = ['Well done!',\n",
    "'Good work',\n",
    "'Great effort',\n",
    "'nice work',\n",
    "'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "#print(t.word_docs)\n",
    "#print(t.word_index)\n",
    "#print(t.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'good': 1, 'work': 2, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n"
     ]
    }
   ],
   "source": [
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n"
     ]
    }
   ],
   "source": [
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(t.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
